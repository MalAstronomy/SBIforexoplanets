{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count the number of parameters\n",
    "def get_n_params(model): \n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np+= p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malavika/anaconda3/envs/petitRT/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/malavika/Documents/Research/assessment/training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"/home/malavika/Documents/Research/assessment/datasets/6params_train_bce_1M2.h5\"\n",
    "#path_to_file = \"/home/malavika/Documents/Research/assessment/datasets/10params_train_bce_1M2.h5\"\n",
    "\n",
    "params = '6'\n",
    "\n",
    "with h5py.File(path_to_file, 'r') as hdf:\n",
    "    data = np.array(hdf.get('data')).astype(np.float32)\n",
    "    label = np.array(hdf.get('labels')).astype(np.float32)\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221852, 377)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "a_min = np.min(data)\n",
    "a_max = np.max(data)\n",
    "data_norm = (data-a_min)/(a_max-a_min)\n",
    "\n",
    "#Tensor\n",
    "target = torch.from_numpy(label)\n",
    "inpt = torch.from_numpy(data_norm)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = inpt\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index] \n",
    "        x = x[np.newaxis, ...] # To specify the number of channels c (here c=1)\n",
    "        y = self.target[index]\n",
    "        \n",
    "        return {'input': x, 'target': y}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [0.9, 0.1]\n",
    "split_train = '0.9'\n",
    "batch_size = 128 \n",
    "indices = list(range(len(dataset)))\n",
    "s = int(np.floor(split[1] * len(dataset)))\n",
    "\n",
    "#shuffling\n",
    "np.random.seed(111)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[s:], indices[:s]\n",
    "\n",
    "train_sampler, val_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([221852, 377])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_dataloader.dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_dataloader.dataset.data)[1]\n",
    "f = np.shape(train_dataloader.dataset.data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(n_feature , n_feature, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(128*42, 128)\n",
    "        self.fc2 = nn.Linear(128,1)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = max_pool1d(x,kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = max_pool1d(x,kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(f, 100) \n",
    "        self.layer_2 = nn.Linear(100, 32)\n",
    "        self.layer_out = nn.Linear(32, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #layer, relu, bn, dp\n",
    "    \n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "                           \n",
    "    def get_weights(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = F.sigmoid(self.fc2(out)) #sigmoid as we use BCELoss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hidden_dim:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        out = F.sigmoid(self.layers[-1](x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_loop.parameters(), lr=float(LEARNING_RATE))\n",
    "optimizer_name = 'Adam'\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "dl_arch = 'mlp'\n",
    "layers= '2hl'\n",
    "metric = 'bce'\n",
    "\n",
    "# summary(model_loop, (1, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'], dataloaders['val'] = train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, sample in enumerate(dataloaders['train']):\n",
    "#     print(len(dataloaders['train'].dataset))\n",
    "#     print(len(dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_'+str(dataset_size)+'spectrum_'+dl_arch+'_'+layers+'_bs'+str(batch_size)+'_lr'+\\\n",
    "    str(LEARNING_RATE)+'_'+str(nb_epoch)+'ep_opt'+str(optimizer_name)+'_split'+split_train+'_'+metric+'_'+params\n",
    "model_path = '/home/malavika/Documents/Research/assessment/models/model1/'\n",
    "model_dir = model_path + model_name \n",
    "if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name\n",
    "#model_221852spectrum_mlp_2hl_bs128_lr0.001_200ep_optAdam_split0.9_bce_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = os.path.join(model_dir, 'metrics.json')\n",
    "    \n",
    "metrics = {\n",
    "    'model': model_dir,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'criterion': criterion.__class__.__name__,\n",
    "#     'scheduler': scheduler.__class__.__name__,\n",
    "    'dataset_size': int(len(dataset)),\n",
    "    'train_size': int(split[0]*len(dataset)),\n",
    "    'test_size': int(split[1]*len(dataset)),\n",
    "    'n_epoch': nb_epoch,\n",
    "    'batch_size': batch_size,\n",
    "#     'learning_rate': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "def train(n_epochs, model):\n",
    "    \n",
    "    best_loss = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            \n",
    "            #both    \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for batch_idx, sample in enumerate(dataloaders[phase]):\n",
    "                inputs = sample['input'].to(device)\n",
    "                target = sample['target'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(torch.squeeze(output), torch.squeeze(target))\n",
    "                    acc = binary_acc(torch.squeeze(output), torch.squeeze(target))\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()   \n",
    "                \n",
    "                    running_loss += 1 * loss.item() * inputs.size(0) #loss for the phase/whole dataset\n",
    "\n",
    "                if batch_idx % 100 == 0: \n",
    "                    print('{} epoch: {} [{}/{} ({:0.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}'.format(\\\n",
    "                    phase,epoch,(batch_idx+1)*len(sample['input']),len(dataloaders[phase].dataset),\\\n",
    "                    100.* ((batch_idx+1)*len(sample['input']))/len(dataloaders[phase].dataset),loss.item(), acc))\n",
    "    \n",
    "            if phase == 'train':\n",
    "                metrics[phase+'_loss'].append(running_loss/int(dataset_size*split[0]))\n",
    "            else:\n",
    "                metrics[phase+'_loss'].append(running_loss/int(dataset_size*split[1]))\n",
    "\n",
    "            if phase == 'val': \n",
    "                if epoch ==  (n_epochs-1) or running_loss < best_loss:\n",
    "                    print('saving')\n",
    "                    best_loss = running_loss\n",
    "                    model_path = os.path.join(model_dir, 'model.pth')\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    \n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "                    \n",
    "#         print('--------------------------------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(nb_epoch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "petitRT",
   "language": "python",
   "name": "petitrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
